{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\cs_bllin001\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to you hugging face account\n",
    "# Hugging Face key\n",
    "with open('../keys/hf_key.txt', 'r') as f:\n",
    "    hf_key = f.read()\n",
    "    login(token = hf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define news NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = dataset[\"train\"].features['pos_tags'].feature.names\n",
    "chunk_list = dataset[\"train\"].features['chunk_tags'].feature.names\n",
    "label_list = dataset[\"train\"].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'B-ATTR',\n",
       " 'I-ATTR',\n",
       " 'B-ACT',\n",
       " 'I-ACT']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tags = ['B-ATTR', 'I-ATTR', 'B-ACT', 'I-ACT']\n",
    "label_list = label_list + new_tags\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS: ['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
      "Chunk: ['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
      "NER: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ATTR', 'I-ATTR', 'B-ACT', 'I-ACT']\n"
     ]
    }
   ],
   "source": [
    "print('POS:', pos_list)\n",
    "print('Chunk:', chunk_list)\n",
    "print('NER:', label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-ADJP': 1,\n",
       " 'I-ADJP': 2,\n",
       " 'B-ADVP': 3,\n",
       " 'I-ADVP': 4,\n",
       " 'B-CONJP': 5,\n",
       " 'I-CONJP': 6,\n",
       " 'B-INTJ': 7,\n",
       " 'I-INTJ': 8,\n",
       " 'B-LST': 9,\n",
       " 'I-LST': 10,\n",
       " 'B-NP': 11,\n",
       " 'I-NP': 12,\n",
       " 'B-PP': 13,\n",
       " 'I-PP': 14,\n",
       " 'B-PRT': 15,\n",
       " 'I-PRT': 16,\n",
       " 'B-SBAR': 17,\n",
       " 'I-SBAR': 18,\n",
       " 'B-UCP': 19,\n",
       " 'I-UCP': 20,\n",
       " 'B-VP': 21,\n",
       " 'I-VP': 22}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index = {chunk_list[i]:i for i in range(len(chunk_list))}\n",
    "chunk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4',\n",
       " 'tokens': ['Germany',\n",
       "  \"'s\",\n",
       "  'representative',\n",
       "  'to',\n",
       "  'the',\n",
       "  'European',\n",
       "  'Union',\n",
       "  \"'s\",\n",
       "  'veterinary',\n",
       "  'committee',\n",
       "  'Werner',\n",
       "  'Zwingmann',\n",
       "  'said',\n",
       "  'on',\n",
       "  'Wednesday',\n",
       "  'consumers',\n",
       "  'should',\n",
       "  'buy',\n",
       "  'sheepmeat',\n",
       "  'from',\n",
       "  'countries',\n",
       "  'other',\n",
       "  'than',\n",
       "  'Britain',\n",
       "  'until',\n",
       "  'the',\n",
       "  'scientific',\n",
       "  'advice',\n",
       "  'was',\n",
       "  'clearer',\n",
       "  '.'],\n",
       " 'pos_tags': [22,\n",
       "  27,\n",
       "  21,\n",
       "  35,\n",
       "  12,\n",
       "  22,\n",
       "  22,\n",
       "  27,\n",
       "  16,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  38,\n",
       "  15,\n",
       "  22,\n",
       "  24,\n",
       "  20,\n",
       "  37,\n",
       "  21,\n",
       "  15,\n",
       "  24,\n",
       "  16,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  12,\n",
       "  16,\n",
       "  21,\n",
       "  38,\n",
       "  17,\n",
       "  7],\n",
       " 'chunk_tags': [11,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  21,\n",
       "  13,\n",
       "  11,\n",
       "  12,\n",
       "  21,\n",
       "  22,\n",
       "  11,\n",
       "  13,\n",
       "  11,\n",
       "  1,\n",
       "  13,\n",
       "  11,\n",
       "  17,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  21,\n",
       "  1,\n",
       "  0],\n",
       " 'ner_tags': [5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample list of dictionaries\n",
    "data = 'train'\n",
    "key_to_filter = 'chunk_tags'\n",
    "desired_values = [1,2]  # List of desired values\n",
    "\n",
    "# Filter the list of dictionaries to keep items where any desired value is found\n",
    "filtered_dataset = [item for item in dataset[data] if any(tag in item.get(key_to_filter, []) for tag in desired_values)]\n",
    "\n",
    "filtered_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_diagram(example, pos_list, chunk_list, label_list):\n",
    "    \n",
    "    pos = [pos_list[i] for i in example[\"pos_tags\"]]\n",
    "    chunk = [chunk_list[i] for i in example[\"chunk_tags\"]]\n",
    "    labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "\n",
    "    print(' '.join(example['tokens']))\n",
    "\n",
    "    for token, pos, chunk, labels in zip(example['tokens'], pos, chunk, labels):\n",
    "        print(f\"{token:-<40} {pos:-<10} {chunk:-<10} {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Germany--------------------------------- NNP------- B-NP------ B-LOC\n",
      "'s-------------------------------------- POS------- B-NP------ O\n",
      "representative-------------------------- NN-------- I-NP------ O\n",
      "to-------------------------------------- TO-------- B-PP------ O\n",
      "the------------------------------------- DT-------- B-NP------ O\n",
      "European-------------------------------- NNP------- I-NP------ B-ORG\n",
      "Union----------------------------------- NNP------- I-NP------ I-ORG\n",
      "'s-------------------------------------- POS------- B-NP------ O\n",
      "veterinary------------------------------ JJ-------- I-NP------ O\n",
      "committee------------------------------- NN-------- I-NP------ O\n",
      "Werner---------------------------------- NNP------- I-NP------ B-PER\n",
      "Zwingmann------------------------------- NNP------- I-NP------ I-PER\n",
      "said------------------------------------ VBD------- B-VP------ O\n",
      "on-------------------------------------- IN-------- B-PP------ O\n",
      "Wednesday------------------------------- NNP------- B-NP------ O\n",
      "consumers------------------------------- NNS------- I-NP------ O\n",
      "should---------------------------------- MD-------- B-VP------ O\n",
      "buy------------------------------------- VB-------- I-VP------ O\n",
      "sheepmeat------------------------------- NN-------- B-NP------ O\n",
      "from------------------------------------ IN-------- B-PP------ O\n",
      "countries------------------------------- NNS------- B-NP------ O\n",
      "other----------------------------------- JJ-------- B-ADJP---- O\n",
      "than------------------------------------ IN-------- B-PP------ O\n",
      "Britain--------------------------------- NNP------- B-NP------ B-LOC\n",
      "until----------------------------------- IN-------- B-SBAR---- O\n",
      "the------------------------------------- DT-------- B-NP------ O\n",
      "scientific------------------------------ JJ-------- I-NP------ O\n",
      "advice---------------------------------- NN-------- I-NP------ O\n",
      "was------------------------------------- VBD------- B-VP------ O\n",
      "clearer--------------------------------- JJR------- B-ADJP---- O\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "example = filtered_dataset[0]\n",
    "sentence_diagram(example, pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign \"ATTR\" tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping, label_all_tokens = True):\n",
    "    \"\"\"\n",
    "    Modify the \"ner\" variable in the example based on the chunk_to_ner_mapping.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): The example dictionary containing \"chunk_tags\" and \"ner_tags\".\n",
    "        chunk_to_ner_mapping (dict): A mapping from chunk tags to ner tags.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The modified example with updated \"ner_tags\".\n",
    "    \"\"\"\n",
    "    for i in range(len(example['tokens'])):\n",
    "        chunk = example['chunk_tags'][i]\n",
    "        if chunk in chunk_to_ner_mapping:\n",
    "            example['ner_tags'][i] = chunk_to_ner_mapping[chunk]\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "chunk_to_ner_mapping = {\n",
    "    1: 9,  # \"B-ADJP\": \"B-ATR\"\n",
    "    2: 10  # \"I-ADJP\": \"I-ATR\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Germany--------------------------------- NNP------- B-NP------ B-LOC\n",
      "'s-------------------------------------- POS------- B-NP------ O\n",
      "representative-------------------------- NN-------- I-NP------ O\n",
      "to-------------------------------------- TO-------- B-PP------ O\n",
      "the------------------------------------- DT-------- B-NP------ O\n",
      "European-------------------------------- NNP------- I-NP------ B-ORG\n",
      "Union----------------------------------- NNP------- I-NP------ I-ORG\n",
      "'s-------------------------------------- POS------- B-NP------ O\n",
      "veterinary------------------------------ JJ-------- I-NP------ O\n",
      "committee------------------------------- NN-------- I-NP------ O\n",
      "Werner---------------------------------- NNP------- I-NP------ B-PER\n",
      "Zwingmann------------------------------- NNP------- I-NP------ I-PER\n",
      "said------------------------------------ VBD------- B-VP------ O\n",
      "on-------------------------------------- IN-------- B-PP------ O\n",
      "Wednesday------------------------------- NNP------- B-NP------ O\n",
      "consumers------------------------------- NNS------- I-NP------ O\n",
      "should---------------------------------- MD-------- B-VP------ O\n",
      "buy------------------------------------- VB-------- I-VP------ O\n",
      "sheepmeat------------------------------- NN-------- B-NP------ O\n",
      "from------------------------------------ IN-------- B-PP------ O\n",
      "countries------------------------------- NNS------- B-NP------ O\n",
      "other----------------------------------- JJ-------- B-ADJP---- B-ATTR\n",
      "than------------------------------------ IN-------- B-PP------ O\n",
      "Britain--------------------------------- NNP------- B-NP------ B-LOC\n",
      "until----------------------------------- IN-------- B-SBAR---- O\n",
      "the------------------------------------- DT-------- B-NP------ O\n",
      "scientific------------------------------ JJ-------- I-NP------ O\n",
      "advice---------------------------------- NN-------- I-NP------ O\n",
      "was------------------------------------- VBD------- B-VP------ O\n",
      "clearer--------------------------------- JJR------- B-ADJP---- B-ATTR\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "# Modify the \"ner_tags\" in the example using the mapping\n",
    "modifed_example = apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping)\n",
    "\n",
    "# Print the modified example\n",
    "sentence_diagram(modifed_example, pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign \"ATR\" entity to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapping_to_split(dataset_split,chunk_to_ner_mapping):\n",
    "    modified_examples = {\n",
    "        'id': [],\n",
    "        'tokens': [],\n",
    "        'pos_tags': [],\n",
    "        'chunk_tags': [],\n",
    "        'ner_tags': []\n",
    "    }\n",
    "    \n",
    "    for example in dataset_split:\n",
    "        modified_example = apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping)\n",
    "        \n",
    "        # Append the modified example to the respective column\n",
    "        modified_examples['id'].append(modified_example['id'])\n",
    "        modified_examples['tokens'].append(modified_example['tokens'])\n",
    "        modified_examples['pos_tags'].append(modified_example['pos_tags'])\n",
    "        modified_examples['chunk_tags'].append(modified_example['chunk_tags'])\n",
    "        modified_examples['ner_tags'].append(modified_example['ner_tags'])\n",
    "    \n",
    "    # Convert the modified examples to a dataset and return it\n",
    "    modified_dataset = datasets.Dataset.from_dict(modified_examples)\n",
    "    return modified_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mapping to each split in the DatasetDict\n",
    "\n",
    "chunk_to_ner_mapping = {\n",
    "    1: 9,  # \"B-ADJP\": \"B-ATR\"\n",
    "    2: 10  # \"I-ADJP\": \"I-ATR\"\n",
    "}\n",
    "\n",
    "# Create a new DatasetDict with the concatenated splits\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': apply_mapping_to_split(dataset['train'], chunk_to_ner_mapping),\n",
    "    'validation': apply_mapping_to_split(dataset['validation'], chunk_to_ner_mapping),\n",
    "    'test': apply_mapping_to_split(dataset['test'], chunk_to_ner_mapping)\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" What we have to be extremely careful of is how other countries are going to take Germany 's lead , \" Welsh National Farmers ' Union ( NFU ) chairman John Lloyd Jones said on BBC radio .\n",
      "\"--------------------------------------- \"--------- O--------- O\n",
      "What------------------------------------ WP-------- B-NP------ O\n",
      "we-------------------------------------- PRP------- B-NP------ O\n",
      "have------------------------------------ VBP------- B-VP------ O\n",
      "to-------------------------------------- TO-------- I-VP------ O\n",
      "be-------------------------------------- VB-------- I-VP------ O\n",
      "extremely------------------------------- RB-------- B-ADJP---- B-ATTR\n",
      "careful--------------------------------- JJ-------- I-ADJP---- I-ATTR\n",
      "of-------------------------------------- IN-------- B-PP------ O\n",
      "is-------------------------------------- VBZ------- B-VP------ O\n",
      "how------------------------------------- WRB------- B-ADVP---- O\n",
      "other----------------------------------- JJ-------- B-NP------ O\n",
      "countries------------------------------- NNS------- I-NP------ O\n",
      "are------------------------------------- VBP------- B-VP------ O\n",
      "going----------------------------------- VBG------- I-VP------ O\n",
      "to-------------------------------------- TO-------- I-VP------ O\n",
      "take------------------------------------ VB-------- I-VP------ O\n",
      "Germany--------------------------------- NNP------- B-NP------ B-LOC\n",
      "'s-------------------------------------- POS------- B-NP------ O\n",
      "lead------------------------------------ NN-------- I-NP------ O\n",
      ",--------------------------------------- ,--------- O--------- O\n",
      "\"--------------------------------------- \"--------- O--------- O\n",
      "Welsh----------------------------------- NNP------- B-NP------ B-ORG\n",
      "National-------------------------------- NNP------- I-NP------ I-ORG\n",
      "Farmers--------------------------------- NNP------- I-NP------ I-ORG\n",
      "'--------------------------------------- POS------- B-NP------ I-ORG\n",
      "Union----------------------------------- NNP------- I-NP------ I-ORG\n",
      "(--------------------------------------- (--------- O--------- O\n",
      "NFU------------------------------------- NNP------- B-NP------ B-ORG\n",
      ")--------------------------------------- )--------- O--------- O\n",
      "chairman-------------------------------- NN-------- B-NP------ O\n",
      "John------------------------------------ NNP------- I-NP------ B-PER\n",
      "Lloyd----------------------------------- NNP------- I-NP------ I-PER\n",
      "Jones----------------------------------- NNP------- I-NP------ I-PER\n",
      "said------------------------------------ VBD------- B-VP------ O\n",
      "on-------------------------------------- IN-------- B-PP------ O\n",
      "BBC------------------------------------- NNP------- B-NP------ B-ORG\n",
      "radio----------------------------------- NN-------- I-NP------ I-ORG\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "sentence_diagram(dataset['train'][16], pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-ADJP': 1,\n",
       " 'I-ADJP': 2,\n",
       " 'B-ADVP': 3,\n",
       " 'I-ADVP': 4,\n",
       " 'B-CONJP': 5,\n",
       " 'I-CONJP': 6,\n",
       " 'B-INTJ': 7,\n",
       " 'I-INTJ': 8,\n",
       " 'B-LST': 9,\n",
       " 'I-LST': 10,\n",
       " 'B-NP': 11,\n",
       " 'I-NP': 12,\n",
       " 'B-PP': 13,\n",
       " 'I-PP': 14,\n",
       " 'B-PRT': 15,\n",
       " 'I-PRT': 16,\n",
       " 'B-SBAR': 17,\n",
       " 'I-SBAR': 18,\n",
       " 'B-UCP': 19,\n",
       " 'I-UCP': 20,\n",
       " 'B-VP': 21,\n",
       " 'I-VP': 22}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index = {chunk_list[i]:i for i in range(len(chunk_list))}\n",
    "chunk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample list of dictionaries\n",
    "data = 'train'\n",
    "key_to_filter = 'chunk_tags'\n",
    "desired_values = [21,22]  # List of desired values\n",
    "\n",
    "# Filter the list of dictionaries to keep items where any desired value is found\n",
    "filtered_dataset = [item for item in dataset[data] if any(tag in item.get(key_to_filter, []) for tag in desired_values)]\n",
    "\n",
    "filtered_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_diagram(example, pos_list, chunk_list, label_list):\n",
    "    \n",
    "    pos = [pos_list[i] for i in example[\"pos_tags\"]]\n",
    "    chunk = [chunk_list[i] for i in example[\"chunk_tags\"]]\n",
    "    labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "\n",
    "    print(' '.join(example['tokens']))\n",
    "\n",
    "    for token, pos, chunk, labels in zip(example['tokens'], pos, chunk, labels):\n",
    "        print(f\"{token:-<40} {pos:-<10} {chunk:-<10} {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU rejects German call to boycott British lamb .\n",
      "EU-------------------------------------- NNP------- B-NP------ B-ORG\n",
      "rejects--------------------------------- VBZ------- B-VP------ O\n",
      "German---------------------------------- JJ-------- B-NP------ B-MISC\n",
      "call------------------------------------ NN-------- I-NP------ O\n",
      "to-------------------------------------- TO-------- B-VP------ O\n",
      "boycott--------------------------------- VB-------- I-VP------ O\n",
      "British--------------------------------- JJ-------- B-NP------ B-MISC\n",
      "lamb------------------------------------ NN-------- I-NP------ O\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "example = filtered_dataset[0]\n",
    "sentence_diagram(example, pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign \"ACT\" tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping, label_all_tokens = True):\n",
    "    \"\"\"\n",
    "    Modify the \"ner\" variable in the example based on the chunk_to_ner_mapping.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): The example dictionary containing \"chunk_tags\" and \"ner_tags\".\n",
    "        chunk_to_ner_mapping (dict): A mapping from chunk tags to ner tags.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The modified example with updated \"ner_tags\".\n",
    "    \"\"\"\n",
    "    for i in range(len(example['tokens'])):\n",
    "        chunk = example['chunk_tags'][i]\n",
    "        if chunk in chunk_to_ner_mapping:\n",
    "            example['ner_tags'][i] = chunk_to_ner_mapping[chunk]\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "chunk_to_ner_mapping = {\n",
    "    21: 11,  # \"B-VP\": \"B-ACT\"\n",
    "    22: 12  # \"I-VP\": \"I-ACT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU rejects German call to boycott British lamb .\n",
      "EU-------------------------------------- NNP------- B-NP------ B-ORG\n",
      "rejects--------------------------------- VBZ------- B-VP------ B-ACT\n",
      "German---------------------------------- JJ-------- B-NP------ B-MISC\n",
      "call------------------------------------ NN-------- I-NP------ O\n",
      "to-------------------------------------- TO-------- B-VP------ B-ACT\n",
      "boycott--------------------------------- VB-------- I-VP------ I-ACT\n",
      "British--------------------------------- JJ-------- B-NP------ B-MISC\n",
      "lamb------------------------------------ NN-------- I-NP------ O\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "# Modify the \"ner_tags\" in the example using the mapping\n",
    "modifed_example = apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping)\n",
    "\n",
    "# Print the modified example\n",
    "sentence_diagram(modifed_example, pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign \"ACT\" entity to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapping_to_split(dataset_split,chunk_to_ner_mapping):\n",
    "    modified_examples = {\n",
    "        'id': [],\n",
    "        'tokens': [],\n",
    "        'pos_tags': [],\n",
    "        'chunk_tags': [],\n",
    "        'ner_tags': []\n",
    "    }\n",
    "    \n",
    "    for example in dataset_split:\n",
    "        modified_example = apply_chunk_to_ner_mapping(example, chunk_to_ner_mapping)\n",
    "        \n",
    "        # Append the modified example to the respective column\n",
    "        modified_examples['id'].append(modified_example['id'])\n",
    "        modified_examples['tokens'].append(modified_example['tokens'])\n",
    "        modified_examples['pos_tags'].append(modified_example['pos_tags'])\n",
    "        modified_examples['chunk_tags'].append(modified_example['chunk_tags'])\n",
    "        modified_examples['ner_tags'].append(modified_example['ner_tags'])\n",
    "    \n",
    "    # Convert the modified examples to a dataset and return it\n",
    "    modified_dataset = datasets.Dataset.from_dict(modified_examples)\n",
    "    return modified_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mapping to each split in the DatasetDict\n",
    "\n",
    "chunk_to_ner_mapping = {\n",
    "    21: 11,  # \"B-VP\": \"B-ACT\"\n",
    "    22: 12  # \"I-VP\": \"I-ACT\"\n",
    "}\n",
    "\n",
    "# Create a new DatasetDict with the concatenated splits\n",
    "modified_dataset = datasets.DatasetDict({\n",
    "    'train': apply_mapping_to_split(dataset['train'], chunk_to_ner_mapping),\n",
    "    'validation': apply_mapping_to_split(dataset['validation'], chunk_to_ner_mapping),\n",
    "    'test': apply_mapping_to_split(dataset['test'], chunk_to_ner_mapping)\n",
    "})\n",
    "modified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan coach Shu Kamo said : ' ' The Syrian own goal proved lucky for us .\n",
      "Japan----------------------------------- NNP------- B-NP------ B-LOC\n",
      "coach----------------------------------- NN-------- I-NP------ O\n",
      "Shu------------------------------------- NNP------- I-NP------ B-PER\n",
      "Kamo------------------------------------ NNP------- I-NP------ I-PER\n",
      "said------------------------------------ VBD------- B-VP------ B-ACT\n",
      ":--------------------------------------- :--------- O--------- O\n",
      "'--------------------------------------- ''-------- O--------- O\n",
      "'--------------------------------------- POS------- B-NP------ O\n",
      "The------------------------------------- DT-------- I-NP------ O\n",
      "Syrian---------------------------------- JJ-------- I-NP------ B-MISC\n",
      "own------------------------------------- JJ-------- I-NP------ O\n",
      "goal------------------------------------ NN-------- I-NP------ O\n",
      "proved---------------------------------- VBD------- B-VP------ B-ACT\n",
      "lucky----------------------------------- JJ-------- B-ADJP---- B-ATTR\n",
      "for------------------------------------- IN-------- B-PP------ O\n",
      "us-------------------------------------- PRP------- B-NP------ O\n",
      ".--------------------------------------- .--------- O--------- O\n"
     ]
    }
   ],
   "source": [
    "sentence_diagram(modified_dataset['test'][16], pos_list, chunk_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload into HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f798cd39a6d347699a54b64e66336d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718c10abadc044d5905b7b73d57f8aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acf2295ddf549b7a05ada1d1ce7a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48c040594724f1aa3e66da39fb64a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019a6e189e594fc2a67fc6fe575a5425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bffa5c2dcc469cb7cac31b23825215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe255801e3423ab075afe4617b7fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e0747b5a984a8fbbea7b6d3b4448ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4e28ae87c4c4ba0412c6492ece199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa77bc6d9794408a8c7116e42799a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_dataset.push_to_hub('storymodelers/dataset-BERT-NER', private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
